# Copyright 2022 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""SlbQuant."""

from functools import partial
import numpy as np
import mindspore
from mindspore import nn
from mindspore.common.parameter import Parameter
from mindspore.ops import operations as P
from mindspore.nn.layer.conv import Conv2d
from mindspore.common.initializer import initializer
from mindspore.common import initializer as init
from mindspore.common.dtype import QuantDtype
from mindspore.common.tensor import Tensor
from mindspore.common import dtype as mstype
from mindspore_gs.validator import Validator, twice
from mindspore_gs.ops.nn.fake_quant_with_min_max_observer import QuantConfig as OpQuantConfig
from mindspore_gs.ops.common.quant_op_utils import get_quant_dtype_num_bits
from mindspore_gs.quantization.quant_cell import QuantCell
from mindspore_gs.quantization.layer_policy import LayerPolicy
from .slb_fake_quantizer import SlbFakeQuantizerPerLayer


quant_config_slb_default = OpQuantConfig(weight=partial(SlbFakeQuantizerPerLayer, num_bits=1),
                                         activation=None)


class Conv2dSlbQuant(QuantCell):
    r"""
    2D convolution with fake quantized operation layer.

    This part is a more detailed overview of Conv2d operation. For more details about Quantization,
    please refer to the implementation of class of `SlbFakeQuantizerPerLayer`,
    :class:`mindspore_gs.quantization.slb.slb_fake_quantizer.SlbFakeQuantizerPerLayer`.

    Args:
        in_channels (int): The number of input channel :math:`C_{in}`.
        out_channels (int): The number of output channel :math:`C_{out}`.
        kernel_size (Union[int, tuple[int]]): Specifies the height and width of the 2D convolution window.
        stride (Union[int, tuple[int]]): Specifies stride for all spatial dimensions with the same value. Default: 1.
        pad_mode (str): Specifies padding mode. The optional values are "same", "valid", "pad". Default: "same".
        padding (Union[int, tuple[int]]): Implicit paddings on both sides of the `x`. Default: 0.
        dilation (Union[int, tuple[int]]): Specifies the dilation rate to use for dilated convolution. Default: 1.
        group (int): Splits filter into groups, `in_ channels` and `out_channels` must be
            divisible by the number of groups. Default: 1.
        has_bias (bool): Specifies whether the layer uses a bias vector. Default: False.
        weight_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the convolution kernel.
            Default: 'normal'.
        bias_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the bias vector. Default: 'zeros'.
        quant_config (QuantConfig): Configures the types of quant observer and quant settings of weight and
            activation. Note that, QuantConfig is a special namedtuple, which is designed for quantization
            and can be generated by :func:`mindspore.compression.quant.create_quant_config` method.
            Default: QuantConfig with weight item set to default :class:`SlbFakeQuantizerPerLayer`,
            and activation item set to None.
        quant_dtype (QuantDtype): Datatype used to quantize weights, weights quantization support int4|int2|int1 now.
            Default: QuantDtype.INT1.

    Inputs:
        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.
          The input dimension is preferably 2D or 4D.

    Outputs:
        Tensor of shape :math:`(N, C_{out}, H_{out}, W_{out})`.

    Raises:
        TypeError: If `in_channels`, `out_channels` or `group` is not an int.
        TypeError: If `kernel_size`, `stride`, `padding` or `dilation` is neither an int nor a tuple.
        TypeError: If `has_bias` is not a bool.
        ValueError: If `in_channels`, `out_channels`, `kernel_size`, `stride` or `dilation` is less than 1.
        ValueError: If `padding` is less than 0.
        ValueError: If `pad_mode` is not one of 'same', 'valid', 'pad'.

    Supported Platforms:
        ``GPU``

    Examples:
        >>> import numpy as np
        >>> import mindspore
        >>> from mindspore_gs.quantization.slb.slb_quant import Conv2dSlbQuant, quant_config_slb_default
        >>> from mindspore import Tensor
        >>> conv2d_quant = Conv2dSlbQuant(1, 1, kernel_size=(2, 2), stride=(1, 1), pad_mode="valid",
        ...                               weight_init='ones', quant_config=quant_config_slb_default)
        >>> x = Tensor(np.array([[[[1, 0, 3], [1, 4, 7], [2, 5, 2]]]]), mindspore.float32)
        >>> result = conv2d_quant(x)
        >>> print(result)
        [[[[-4.  -8.]
           [-2.  4.]]]]
    """

    def __init__(self,
                 handler: nn.Cell,
                 policy: LayerPolicy,
                 in_channels,
                 out_channels,
                 kernel_size,
                 stride=1,
                 pad_mode='same',
                 padding=0,
                 dilation=1,
                 group=1,
                 has_bias=False,
                 weight_init='normal',
                 bias_init='zeros',
                 quant_config=quant_config_slb_default,
                 weight_quant_dtype=QuantDtype.INT1):
        """Initialize Conv2dSlbQuant."""
        super(Conv2dSlbQuant, self).__init__(handler, policy)
        self.in_channels = Validator.check_positive_int(in_channels, "in_channels", self.cls_name)
        self.out_channels = Validator.check_positive_int(out_channels, "out_channels", self.cls_name)
        self.has_bias = has_bias
        self.kernel_size = twice(kernel_size)
        self.stride = twice(stride)
        self.dilation = twice(dilation)
        for kernel_size_elem in self.kernel_size:
            Validator.check_positive_int(kernel_size_elem, 'kernel_size item', self.cls_name)
        for stride_elem in self.stride:
            Validator.check_positive_int(stride_elem, 'stride item', self.cls_name)
        for dilation_elem in self.dilation:
            Validator.check_positive_int(dilation_elem, 'dilation item', self.cls_name)
        if pad_mode not in ('valid', 'same', 'pad'):
            raise ValueError(f"For '{self.cls_name}', the 'pad_mode' must be one of values "
                             f"in ('valid', 'same', 'pad'), but got {pad_mode}.")
        self.pad_mode = pad_mode
        if isinstance(padding, int):
            Validator.check_non_negative_int(padding, 'padding', self.cls_name)
            self.padding = padding
        elif isinstance(padding, tuple):
            for pad in padding:
                Validator.check_non_negative_int(pad, 'padding item', self.cls_name)
            self.padding = padding
        else:
            raise TypeError(f"For '{self.cls_name}', the type of 'padding' must be int/tuple(int), "
                            f"but got {type(padding).__name__}!")
        self.group = Validator.check_positive_int(group, "group", self.cls_name)

        self._weight_num_bits = get_quant_dtype_num_bits(weight_quant_dtype)
        self._weight_num = 2**self._weight_num_bits

        weight_init = init.HeNormal(mode='fan_out', nonlinearity='relu')
        weight_shape = [out_channels, in_channels // group, *self.kernel_size, self._weight_num]
        self.weight = Parameter(initializer(weight_init, weight_shape, mindspore.float32),
                                name='weight', requires_grad=True)


        self.bias_add = P.BiasAdd()
        if Validator.check_bool(has_bias, "has_bias", self.cls_name):
            self.bias = Parameter(initializer(bias_init, [out_channels]), name='bias')
        else:
            self.bias = None

        self.conv = P.Conv2D(out_channel=self.out_channels,
                             kernel_size=self.kernel_size,
                             mode=1,
                             pad_mode=self.pad_mode,
                             pad=self.padding,
                             stride=self.stride,
                             dilation=self.dilation,
                             group=self.group)

        self._weight_quantizer = quant_config.weight()

    @classmethod
    def from_float(cls, conv: Conv2d, quant_config: OpQuantConfig, weight_quant_dtype: QuantDtype,
                   layer_policy: LayerPolicy = None):
        """
        A class method to create `Conv2dSlbQuant` from a `Conv2d`

        Examples:
            >>> from functools import partial
            >>> from mindspore import nn
            >>> from mindspore.nn.layer.quant import QuantConfig as OpQuantConfig
            >>> from mindspore_gs.quantization.slb.slb_quant import Conv2dSlbQuant
            >>> from mindspore_gs.quantization.slb.slb_fake_quantizer import SlbFakeQuantizerPerLayer
            >>> from mindspore.common.dtype import QuantDtype
            >>> ic = 10
            >>> oc = 100
            >>> kernel_size = 3
            >>> conv_op = nn.Conv2d(ic, oc, kernel_size)
            >>> # when apply QAT on `conv_op`, QAT need to create a quant conv2d whose weight is fake-quanted
            >>> quant_config: OpQuantConfig = OpQuantConfig(weight=partial(SlbFakeQuantizerPerLayer, num_bits=1),
            >>>                                             activation=None)
            >>> weight_quant_dtype: QuantDtype = QuantDtype.INT1
            >>> conv_quant = Conv2dSlbQuant.from_float(conv_op, quant_config, weight_quant_dtype)
        """
        conv_quant = cls(
            conv,
            layer_policy,
            conv.in_channels,
            conv.out_channels,
            kernel_size=conv.kernel_size,
            stride=conv.stride,
            pad_mode=conv.pad_mode,
            padding=conv.padding,
            dilation=conv.dilation,
            group=conv.group,
            has_bias=conv.has_bias,
            bias_init=conv.bias_init,
            weight_init=conv.weight_init,
            quant_config=quant_config,
            weight_quant_dtype=weight_quant_dtype)
        return conv_quant

    def weight_quantizer(self):
        return self._weight_quantizer

    def __convert_weight5d_to_weight4d(self, conv2dquant_weights5d):
        """Convert slb 5d weight to normal 4d weight"""
        argmax = P.Argmax()
        onehot = P.OneHot()
        reduce_sum = P.ReduceSum()
        true_tensor = Tensor(1, mstype.float32)
        false_tensor = Tensor(0, mstype.float32)
        num_bits = self._weight_num_bits

        if num_bits == 1:
            w_list = Parameter(Tensor([-1, 1], mstype.float32).view(1, 1, 1, 1, -1),
                               name='w_list', requires_grad=False)
        else:
            w_list_init = np.linspace(-1, 1, 2**num_bits)
            w_list = Parameter(Tensor(w_list_init, mstype.float32).view(1, 1, 1, 1, -1),
                               name='w_list', requires_grad=False)

        # Convert 5d weight to 4d weight
        # Compute one-hot representation of matrix A's argmax
        onehot_weights5d = onehot(argmax(conv2dquant_weights5d), conv2dquant_weights5d.shape[-1],
                                  true_tensor, false_tensor)
        # Compute continuous weights
        weights_5d = onehot_weights5d * w_list
        weights_4d = reduce_sum(weights_5d, -1)
        return weights_4d

    def convert(self):
        super(Conv2dSlbQuant, self).convert()
        self._weight_quantizer = self._weight_quantizer.convert_to_fakequantparam()
        weight_tensor = self.__convert_weight5d_to_weight4d(self.weight)
        self.weight = Parameter(weight_tensor, name=f"{self.weight.name}_4d")

    # pylint: disable=arguments-differ
    def core_construct(self, x):
        weight = self._weight_quantizer(self.weight)
        out = self.conv(x, weight)
        if self.has_bias:
            return self.bias_add(out, self.bias)
        return out

    def extend_repr(self):
        """Display instance object as string."""
        s = 'in_channels={}, out_channels={}, kernel_size={}, weight_bit_num={}, stride={}, ' \
            'pad_mode={}, padding={}, dilation={}, group={}, ' \
            'has_bias={}'.format(self.in_channels, self.out_channels, self.kernel_size, self._weight_num_bits,
                                 self.stride, self.pad_mode, self.padding, self.dilation, self.group, self.has_bias)
        return s
