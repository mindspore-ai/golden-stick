# Copyright 2022 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""Conv2dBnFoldQuantOneConv."""
from __future__ import absolute_import

import mindspore.context as context
import mindspore.common.dtype as mstype
from mindspore import Tensor
from mindspore.ops import operations as P
from mindspore.common.parameter import Parameter
from mindspore.common.dtype import QuantDtype
from mindspore_gs.common import BackendTarget
from mindspore_gs.validator import Validator
from mindspore_gs.quantization.simulated_quantization.combined import Conv2dBn
from mindspore_gs.quantization.quant_cell import QuantCell
from mindspore_gs.quantization.layer_policy import LayerPolicy, PerChannelArgs
from mindspore_gs.quantization.quant_utils import fold_batchnorm


class Conv2dBnFoldQuantOneConv(QuantCell):
    r"""
    2D convolution which use the convolution layer statistics once to calculate Batch Normalization
    operation folded construct.

    This part is a more detailed overview of Conv2d operation. For more details about Quantization,
    please refer to the implementation of class of `FakeQuantWithMinMaxObserver`,
    :class:`FakeQuantWithMinMaxObserver`.

    .. math::
        w_{q}=quant(\frac{w}{\sqrt{var_{G}+\epsilon}}*\gamma )

        b=\frac{-\mu _{G} }{\sqrt{var_{G}+\epsilon }}*\gamma +\beta

        y=w_{q}\times x+b

    where :math:`quant` is the continuous execution of quant and dequant, you can refer to the implementation of
    subclass of `FakeQuantWithMinMaxObserver`, :class:`mindspore.nn.FakeQuantWithMinMaxObserver`.
    `mu _{G}` and `var_{G}` represent the global mean and variance respectively.

    Args:
        in_channels (int): The number of input channel :math:`C_{in}`.
        out_channels (int): The number of output channel :math:`C_{out}`.
        kernel_size (Union[int, tuple[int]]): Specifies the height and width of the 2D convolution window.
        stride (Union[int, tuple[int]]): Specifies stride for all spatial dimensions with the same value. Default: 1.
        pad_mode (str): Specifies padding mode. The optional values are "same", "valid", "pad". Default: "same".
        padding (Union[int, tuple[int]]): Implicit paddings on both sides of the `x`. Default: 0.
        dilation (Union[int, tuple[int]]): Specifies the dilation rate to use for dilated convolution. Default: 1.
        group (int): Splits filter into groups, `in_channels` and `out_channels` must be
            divisible by the number of groups. Default: 1.
        eps (float): Parameters for Batch Normalization. Default: 1e-5.
        momentum (float): Parameters for Batch Normalization op. Default: 0.997.
        has_bias (bool): Specifies whether the layer uses a bias vector, which is temporarily invalid. Default: False.
        weight_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the
            convolution kernel. Default: 'normal'.
        bias_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the
            bias vector. Default: 'zeros'.
        beta_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the
            beta vector. Default: 'zeros'.
        gamma_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the
            gamma vector. Default: 'ones'.
        mean_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the
            mean vector. Default: 'zeros'.
        var_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the
            variance vector. Default: 'ones'.
        fake (bool): Whether Conv2dBnFoldQuant Cell adds FakeQuantWithMinMaxObserver. Default: True.
        quant_config (QuantConfig): Configures the types of quant observer and quant settings of weight and
            activation. Note that, QuantConfig is a special namedtuple, which is designed for quantization
            and can be generated by :func:`mindspore.compression.quant.create_quant_config` method.
            Default: QuantConfig with both items set to default :class:`FakeQuantWithMinMaxObserver`.
        quant_dtype (QuantDtype): Specifies the FakeQuant datatype. Default: QuantDtype.INT8.

    Inputs:
        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.

    Outputs:
        Tensor of shape :math:`(N, C_{out}, H_{out}, W_{out})`.

    Raises:
        TypeError: If `in_channels`, `out_channels` or `group` is not an int.
        TypeError: If `kernel_size`, `stride`, `padding` or `dilation` is neither an int nor a tuple.
        TypeError: If `has_bias` or `fake` is not a bool.
        TypeError: If `data_format` is not a string.
        ValueError: If `in_channels`, `out_channels`, `kernel_size`, `stride` or `dilation` is less than 1.
        ValueError: If `padding` is less than 0.
        ValueError: If `pad_mode` is not one of 'same', 'valid', 'pad'.

    Supported Platforms:
        ``Ascend`` ``GPU``

    Examples:
        >>> import numpy as np
        >>> import mindspore
        >>> from mindspore import Tensor, nn
        >>> conv2d_bnfold = nn.Conv2dBnFoldQuantOneConv(1, 1, kernel_size=(2, 2), stride=(1, 1), pad_mode="valid",
        ...                                             weight_init="ones")
        >>> x = Tensor(np.array([[[[1, 0, 3], [1, 4, 7], [2, 5, 2]]]]), mindspore.float32)
        >>> result = conv2d_bnfold(x)
        >>> print(result)
        [[[[5.9296875 13.8359375]
           [11.859375 17.78125]]]]
    """

    def __init__(self, handler: Conv2dBn, policy: LayerPolicy, fake=True, quant_dtype=QuantDtype.INT8):
        """Initialize Conv2dBnFoldQuant layer"""
        if not handler.has_bn:
            raise ValueError(f"For '{self.cls_name}', input Conv2dBn should has batchnorm.")
        super(Conv2dBnFoldQuantOneConv, self).__init__(handler, policy)
        self.in_channels = handler.in_channels
        self.out_channels = handler.out_channels
        self.kernel_size = handler.kernel_size
        self.stride = handler.stride
        self.dilation = handler.dilation
        self.pad_mode = handler.pad_mode
        self.padding = handler.padding
        self.group = handler.group
        self.has_bias = handler.has_bias
        self.eps = handler.batchnorm.eps
        self.momentum = 1 - handler.batchnorm.momentum
        self.fake = Validator.check_bool(fake, "fake", self.cls_name)
        self.format = 'NCHW'
        self._target = context.get_context("device_target")
        self.is_graph_mode = context.get_context("mode") == context.GRAPH_MODE
        self.is_ge_backend = False
        if context.get_context("enable_ge"):
            self.is_ge_backend = True
        self.enable_default_train = self.is_graph_mode and (self.is_ge_backend or self._target == "Ascend")

        # initialize convolution op and Parameter
        self.conv = P.Conv2D(out_channel=self.out_channels,
                             kernel_size=self.kernel_size,
                             pad_mode=self.pad_mode,
                             pad=self.padding,
                             stride=self.stride,
                             dilation=self.dilation,
                             group=self.group)
        channel_axis = 0
        self.channel_axis = channel_axis
        self.weight = handler.weight
        self.bias_add = P.BiasAdd()
        if self.has_bias:
            self.bias = handler.bias

        # initialize BatchNorm Parameter
        self.gamma = handler.batchnorm.gamma
        self.beta = handler.batchnorm.beta
        self.moving_mean = handler.batchnorm.moving_mean
        self.moving_variance = handler.batchnorm.moving_variance

        # initialize fake ops
        weight_perchannel_args = PerChannelArgs(self.out_channels, channel_axis)
        self._weight_quantizer = policy.get_weight_quantizer(self.weight.name, weight_perchannel_args)
        self.freeze_bn = False
        self.bn_train = P.BatchNorm(is_training=True, epsilon=self.eps, momentum=self.momentum, data_format=self.format)

        self.bn_infer = P.BatchNorm(is_training=False, epsilon=self.eps, data_format=self.format)
        self.sub_mean = P.Sub()
        self.sub_var = P.Sub()
        self.mul_mean = P.Mul()
        self.mul_var = P.Mul()
        self.assign_sub_mean = P.AssignSub()
        self.assign_sub_var = P.AssignSub()
        self.reshape = P.Reshape()
        _ = quant_dtype  # for fix pylint unused-argument

    def weight_quantizer(self):
        return self._weight_quantizer

    def extend_repr(self):
        """Display instance object as string."""
        s = 'in_channels={}, out_channels={}, kernel_size={}, stride={}, ' \
            'pad_mode={}, padding={}, dilation={}, group={}, ' \
            'fake={}, momentum={}'.format(self.in_channels, self.out_channels, self.kernel_size, self.stride,
                                          self.pad_mode, self.padding, self.dilation, self.group, self.fake,
                                          self.momentum)
        return s

    def convert(self, backend: BackendTarget = BackendTarget.NONE, is_deploy=False):
        if self._converted:
            return
        if backend is not BackendTarget.NONE:
            raise ValueError("Only support convert to MS Backend now, got: ", backend)
        if self.has_bias and self.bias:
            raise ValueError("Only support conv2d with out bias.")
        super(Conv2dBnFoldQuantOneConv, self).convert(backend, is_deploy)
        self._weight_quantizer = self._weight_quantizer.convert_to_fakequantparam()
        weight, bias = fold_batchnorm(self.weight.data.asnumpy(), self)
        weight_tensor = Tensor(weight)
        bias_tensor = Tensor(bias, mstype.float32)
        self.weight = Parameter(weight_tensor, name=f"{self.weight.name}_bnfold")
        bias_name = f"{self.weight.name}_bias_bnfold"
        self.bias = Parameter(bias_tensor, name=bias_name)
        self.has_bias = True

    # pylint: disable=arguments-differ
    def core_construct(self, x):
        """construct."""
        running_std = P.Sqrt()(P.Add()(self.moving_variance, self.eps))
        scale_factor = self.gamma / running_std
        if self.channel_axis:
            scale_factor = self.reshape(scale_factor, (1, -1, 1, 1))
        else:
            scale_factor = self.reshape(scale_factor, (-1, 1, 1, 1))
        weight = self.weight * scale_factor
        if self.fake:
            weight = self._weight_quantizer(weight)
        conv = self.conv(x, weight)

        if self.freeze_bn:
            return conv + self.reshape((self.beta - self.gamma * self.moving_mean / running_std), (1, -1, 1, 1))
        scale_factor = self.reshape(scale_factor, (1, -1, 1, 1))
        if self.enable_default_train:
            scale_factor = P.Reciprocal()(scale_factor)
            conv_orig = conv * scale_factor
        else:
            conv_orig = conv / scale_factor
        if self.training:
            return self.bn_train(conv_orig,
                                 self.gamma,
                                 self.beta,
                                 self.moving_mean,
                                 self.moving_variance)[0]

        return self.bn_infer(conv_orig,
                             self.gamma,
                             self.beta,
                             self.moving_mean,
                             self.moving_variance)[0]
