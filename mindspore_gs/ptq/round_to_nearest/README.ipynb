{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 应用RoundToNearest后量化算法\n",
    "\n",
    "[![查看源文件](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source.svg)](https://gitee.com/mindspore/golden-stick/blob/r0.4/mindspore_gs/ptq/round_to_nearest/README.ipynb)\n",
    "\n",
    "## RoundToNearest后量化算法简介\n",
    "\n",
    "RoundToNearest算法是一类较朴素的后量化算法，其取整方式使用了Round to nearest，即四舍五入的方式。\n",
    "\n",
    "当前金箍棒中的RoundToNearest后量化（后面使用RTN来简称）主要针对LLM（大语言模型场景），使用MinMax校正器对线性层（Linear）进行量化。伪量化的网络结构示意如下：\n",
    "\n",
    "![fakequantizer](https://gitee.com/mindspore/golden-stick/raw/r0.4/docs/images/ptq/rtn/rtn-fakequantizer.png)\n",
    "\n",
    "表1：RTN算法规格\n",
    "\n",
    "| 规格 | 规格说明 |\n",
    "| --- | --- |\n",
    "| 硬件支持 | 量化阶段运行在CPU，量化模型推理仅支持Ascend |\n",
    "| 网络支持 | Llama2 13B/70B，具体请参见[Llama2网络](https://gitee.com/hangangqiang/mindformers/tree/dev/mindformers/models/llama) |\n",
    "| 运行模式支持 | Graph模式和PyNative模式 |\n",
    "\n",
    "## 示例\n",
    "\n",
    "跟金箍棒仓所有算法一样，RTN算法的应用主要可以分为两个阶段：量化阶段和部署阶段。\n",
    "\n",
    "量化阶段是部署前提前完成的，主要的工作是：收集权重的分布、计算量化参数、量化权重数据、插入反量化节点。\n",
    "\n",
    "部署阶段通常是指用户在生产环境，使用MindSpore框架对量化后的模型进行推理的过程。\n",
    "\n",
    "本用例使用Llama2网络进行演示，主要分三个步骤：环境准备、模型量化、模型部署并评估。\n",
    "\n",
    "### 步骤1. 环境准备\n",
    "\n",
    "#### 1.1. Ascend环境\n",
    "\n",
    "RTN算法需要运行在Ascend硬件上，Ascend的环境配置可以参考[MindSpore安装指南](https://www.mindspore.cn/install)安装昇腾AI处理器配套软件包小节和配置环境变量小节。\n",
    "\n",
    "#### 1.2. MindSpore环境\n",
    "\n",
    "金箍棒依赖于MindSpore，需要提前安装合适的MindSpore。可以从MindSpore官网下载预编译好的[v2.3版本安装包](https://www.mindspore.cn/versions#2.3)并安装。\n",
    "\n",
    "#### 1.3. MindSpore Transformers环境\n",
    "\n",
    "金箍棒依赖于MindSpore Transformers，需要提前安装合适的MindSpore Transformers。可以从MindSpore官网下载预编译好的[v1.?版本安装包](https://www.mindspore.cn/versions#2.3)并安装。\n",
    "\n",
    "#### 1.4. 金箍棒环境\n",
    "\n",
    "从MindSpore官网下载预编译好的[MindSpore GoldenStick v0.4.0 版本安装包](https://www.mindspore.cn/versions#2.3)并安装。\n",
    "\n",
    "#### 1.5. 相关文件准备\n",
    "\n",
    "需要预先下载MindSpore Transformers Llama2网络相关的文件以及评估使用的数据集，包括：Llama2 13B网络checkpoint文件，Llama2分词器文件，Llama2模型配置文件，wikitext2 datasets："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!cd workspace; wget -O tokenizer.model https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/MindFormers/llama2/tokenizer.model\n",
    "!cd workspace; wget -O llama2-13b-fp16.ckpt https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/MindFormers/llama2/llama2_7b.ckpt\n",
    "!cd workspace; cp golden-stick/mindspore_gs/ptq/round_to_nearest/configs/run_llama2_7b_910b.yaml ./\n",
    "!cd workspace; wget -O wikitext-2-v1.zip https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\n",
    "!cd workspace; unzip wikitext-2-v1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 下载时如果遇到网络问题，可以尝试：\n",
    "> 1. 为wget添加 --no-check-certificate 参数\n",
    "> 2. 使用浏览器手动下载相应文件，并放到相应目录下\n",
    "\n",
    "完成所有准备后，检查目录结构："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!cd workspace; tree -L 2 -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当前目录下至少应该包括下述文件：\n",
    "\n",
    "```bash\n",
    ".\n",
    "├── llama2_7b.ckpt\n",
    "├── tokenizer.model\n",
    "├── wikitext-2-v1.zip\n",
    "├── wikitext-2\n",
    "│   ├── wiki.valid.tokens\n",
    "│   ├── wiki.test.tokens\n",
    "│   └── wiki.train.tokens\n",
    "├── ...\n",
    "```\n",
    "\n",
    "### 步骤2. 模型量化\n",
    "\n",
    "#### 2.1. 实例化MindFormerConfig\n",
    "\n",
    "构造MindSpore Transformers仓的Llama2网络，首先需要构造MindFormerConfig配置项，本示例中，先编写了一个构造MindFormerConfig的函数，并实例化一个MindFormerConfig对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "from mindspore import context\n",
    "from mindformers import LlamaForCausalLM, MindFormerConfig, LlamaConfig, init_context, TransformerOpParallelConfig\n",
    "\n",
    "\n",
    "def _set_config(config_path, device_target, device_id):\n",
    "    \"\"\"setup MindFormerConfig\"\"\"\n",
    "    mfconfig = MindFormerConfig(config_path)\n",
    "    if device_id != -1:\n",
    "        mfconfig.context.device_id = device_id\n",
    "    mfconfig.context.device_target = device_target\n",
    "    mfconfig.model.model_config = LlamaConfig(**mfconfig.model.model_config)\n",
    "\n",
    "    init_context(use_parallel=mfconfig.use_parallel, context_config=mfconfig.context, parallel_config=mfconfig.parallel)\n",
    "\n",
    "    parallel_config = TransformerOpParallelConfig(**mfconfig.parallel_config)\n",
    "    mfconfig.model.model_config.parallel_config = parallel_config\n",
    "    mfconfig.model.model_config.checkpoint_name_or_path = mfconfig.load_checkpoint\n",
    "    return mfconfig\n",
    "\n",
    "\n",
    "def create_mfconfig(config_path, device_target, device_id, bs, seq_len, tokenizer_path=\"\", ckpt_path=\"\", model_parallel=1):\n",
    "    \"\"\"Create mindformers config for llama2 network for example.\"\"\"\n",
    "    if model_parallel > 1:\n",
    "        use_parallel = True\n",
    "    else:\n",
    "        use_parallel = False\n",
    "        model_parallel = 1\n",
    "    compute_dtype = ms.float16\n",
    "    config = _set_config(config_path, device_target, device_id)\n",
    "    config.model.model_config.batch_size = bs\n",
    "    config.model.model_config.seq_length = seq_len\n",
    "    config.model.model_config.compute_dtype = compute_dtype\n",
    "    config.model.model_config.layernorm_compute_type = ms.float32\n",
    "    config.model.model_config.softmax_compute_type = ms.float16\n",
    "    config.model.model_config.rotary_dtype = ms.float32\n",
    "    config.model.model_config.param_init_type = ms.float32\n",
    "    config.processor.tokenizer.vocab_file = tokenizer_path\n",
    "    config.load_checkpoint = ckpt_path\n",
    "    config.model.model_config.checkpoint_name_or_path = ckpt_path\n",
    "    config.use_parallel = use_parallel\n",
    "    config.parallel_config.model_parallel = model_parallel\n",
    "    return config\n",
    "\n",
    "context.set_context(device_target=\"CPU\", mode=ms.GRAPH_MODE)\n",
    "llama2_config_file = \"./workspace/run_llama2_7b_910b.yaml\"\n",
    "llama2_w16a16_ckpt_file = \"./workspace/llama2_7b.ckpt\"\n",
    "llama2_w8a16_ckpt_file = \"./workspace/llama2-7b-w8a16.ckpt\"\n",
    "vocab_file = \"./workspace/tokenizer.model\"\n",
    "wikitext2_ds_path = \"./workspace/wikitext-2/wiki.valid.tokens\"\n",
    "bs = 1\n",
    "seq_len = 256\n",
    "device_id = 0\n",
    "\n",
    "quant_network_config = create_mfconfig(llama2_config_file, \"CPU\", device_id, bs, seq_len, ckpt_path=llama2_w16a16_ckpt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 代码中的device_id变量可以根据运行环境中Ascend硬件空闲情况修改。\n",
    "\n",
    "#### 2.2. 实例化Llama2网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "from mindformers import LlamaForCausalLM\n",
    "\n",
    "network = LlamaForCausalLM(quant_network_config.model.model_config)\n",
    "network.set_train(False)\n",
    "network.phase = 'predict'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. 实例化RTN算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore_gs.common.gs_enum import PTQMode, BackendTarget\n",
    "from mindspore_gs.ptq import PTQConfig\n",
    "from mindspore_gs.ptq import RoundToNearest as RTN\n",
    "cfg = PTQConfig(mode=PTQMode.QUANTIZE, backend=BackendTarget.ASCEND)\n",
    "ptq = RTN(config=cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. 量化Llama2网络并保存ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet = ptq.apply(network.model)\n",
    "qnet = ptq.convert(qnet)\n",
    "network.model = qnet\n",
    "ms.save_checkpoint(network, llama2_w8a16_ckpt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "成功运行后，会在当前目录下生成`llama2-7b-w8a16.ckpt`文件。\n",
    "\n",
    "### 步骤3. 模型部署\n",
    "\n",
    "#### 3.1. 实例化MindFormerConfig和Llama2网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.set_context(device_target=\"Ascend\")\n",
    "deploy_network_config = create_mfconfig(llama2_config_file, \"Ascend\", device_id, bs, seq_len, ckpt_path=llama2_w8a16_ckpt_file)\n",
    "deploy_network = LlamaForCausalLM(deploy_network_config.model.model_config)\n",
    "deploy_network.set_train(False)\n",
    "deploy_network.phase = 'predict'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. 加载量化后的ckpt\n",
    "\n",
    "由于MindSpore当前不支持保存修改后的网络，所以在加载量化ckpt之前，需要先用算法恢复带量化结构的网络，然后再加载ckpt到网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_cfg = PTQConfig(mode=PTQMode.DEPLOY, backend=BackendTarget.ASCEND)\n",
    "deploy_ptq = RTN(config=deploy_cfg)\n",
    "deploy_network.model = deploy_ptq.apply(deploy_network.model)\n",
    "deploy_network.model = deploy_ptq.convert(deploy_network.model)\n",
    "ms.load_checkpoint(llama2_w8a16_ckpt_file, deploy_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. 评估量化后的网络\n",
    "\n",
    "本示例对Llama2在wikitext2数据集上评估Perplexity指标。使用步骤1中下载好的分词器和数据集文件分别实例化分词器对象和数据集对象，并实例化PerplexityMetric对象作为metric。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore_gs.datasets import create_wikitext_dataset\n",
    "from mindformers import LlamaTokenizer\n",
    "from mindformers.core.metric import PerplexityMetric\n",
    "\n",
    "tokenizer = LlamaTokenizer(vocab_file=vocab_file)\n",
    "deploy_ds = create_wikitext_dataset(wikitext2_ds_path, bs, seq_len, tokenizer)\n",
    "deploy_metrics = {\"PerplexityMetric\": PerplexityMetric()}\n",
    "deploy_model = ms.Model(deploy_network, metrics=deploy_metrics, eval_network=deploy_network)\n",
    "quant_ppl = deploy_model.eval(deploy_ds, dataset_sink_mode=deploy_network_config.runner_config.sink_mode)\n",
    "print(f\"W8A16 Perplexity: {quant_ppl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到结果：W8A16 Perplexity: {'PerplexityMetric': {'loss': 2.9108531734988285, 'PPL': 18.372466783981967}}\n",
    "\n",
    "#### 3.4. 评估FP16网络的Perplexity指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp16_network_config = create_mfconfig(llama2_config_file, \"Ascend\", device_id, bs, seq_len, ckpt_path=llama2_w16a16_ckpt_file)\n",
    "fp16_network = LlamaForCausalLM(fp16_network_config.model.model_config)\n",
    "fp16_network.set_train(False)\n",
    "fp16_network.phase = 'predict'\n",
    "ms.load_checkpoint(llama2_w16a16_ckpt_file, fp16_network)\n",
    "fp16_ds = create_wikitext_dataset(wikitext2_ds_path, bs, seq_len, tokenizer)\n",
    "fp16_metrics = {\"PerplexityMetric\": PerplexityMetric()}\n",
    "fp16_model = ms.Model(fp16_network, metrics=fp16_metrics, eval_network=fp16_network)\n",
    "fp16_ppl = fp16_model.eval(fp16_ds, dataset_sink_mode=fp16_network_config.runner_config.sink_mode)\n",
    "print(f\"FP16 Perplexity: {fp16_ppl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到结果：FP16 Perplexity: {'PerplexityMetric': {'loss': 3.0012421822877363, 'PPL': 18.581457892192436}}\n",
    "\n",
    "#### 3.5. 比较结果\n",
    "\n",
    "表2：Llama2 7B网络RTN算法量化前后对比\n",
    "\n",
    "| 指标 | FP16 | W8A16 | 收益 |\n",
    "| --- | --- | --- | --- |\n",
    "| ckpt-size(GB)↓ | 13 | 7.1 | -5.9 |\n",
    "| wikitext2-Perplexity↓ | 18.581 | 18.372 | -0.209 |\n",
    "\n",
    "可以看到，经过RTN量化算法处理后：\n",
    "\n",
    "1. 网络的参数量缩减了5.9GB，只剩下原Float16时的54.6%，即网络部署时，用于静态权重存储的显存下降到Float16时的54.6%。因而量化后的网络可以在资源更紧张的环境上部署，或者在相同的环境中提供更大的吞吐量。\n",
    "2. 网络的在wikitext2数据集上的混淆度有略微下降，即网络在wikitext2上生成式任务的效果反而有所提升。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
